[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen heavy alchohol consumption, healthcare, ability/inability to see a physicians due to the cost of the visit, education and income as the categorical variables and mental and physical health as the continuous variables. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen heavy alchohol consumption, healthcare, ability/inability to see a physicians due to the cost of the visit, education and income as the categorical variables and mental and physical health as the continuous variables. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#initial-library-read-in",
    "href": "EDA.html#initial-library-read-in",
    "title": "ST558 - Final Project: EDA",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(Rmisc)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)"
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "ST558 - Final Project: EDA",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_data &lt;- read_csv(\"./data/diabetes_binary_health_indicators_BRFSS2015.csv\", lazy = FALSE)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n#checking on missing values\nsum(is.na(diabetes_data))\n\n[1] 0\n\n\n\n#saving the edited data set for future use\nwrite_csv(diabetes_data, \"./data/diabetes_data.csv\")\n\n\n#getting an idea of the data set\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              Yes    Yes      Yes          40 Yes    No    \n 2 No              No     No       No           25 Yes    No    \n 3 No              Yes    Yes      Yes          28 No     No    \n 4 No              Yes    No       Yes          27 No     No    \n 5 No              Yes    Yes      Yes          24 No     No    \n 6 No              Yes    Yes      Yes          25 Yes    No    \n 7 No              Yes    No       Yes          30 Yes    No    \n 8 No              Yes    Yes      Yes          25 Yes    No    \n 9 Yes             Yes    Yes      Yes          30 Yes    No    \n10 No              No     No       Yes          24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;"
  },
  {
    "objectID": "EDA.html#summaries",
    "href": "EDA.html#summaries",
    "title": "ST558 - Final Project: EDA",
    "section": "Summaries:",
    "text": "Summaries:\n\n#getting summary of selected variables, will be useful later on\nselected_cols &lt;- c(\"Diabetes_binary\", \"AnyHealthcare\", \"NoDocbcCost\", \"Education\", \"Income\", \"MentHlth\", \"PhysHlth\")\n\nsummary(diabetes_data[selected_cols])\n\n Diabetes_binary AnyHealthcare NoDocbcCost                       Education     \n No :218334      No : 12417    No :232326   None                      :   174  \n Yes: 35346      Yes:241263    Yes: 21354   Elementary                :  4043  \n                                            Middle.School             :  9478  \n                                            High.School               : 62750  \n                                            Some.or.Technnical.College: 69910  \n                                            College.Graduate          :107325  \n                                                                               \n       Income         MentHlth         PhysHlth     \n &gt;.$75,000:90385   Min.   : 0.000   Min.   : 0.000  \n &lt;.$75,000:43219   1st Qu.: 0.000   1st Qu.: 0.000  \n &lt;.$50,000:36470   Median : 0.000   Median : 0.000  \n &lt;.$35,000:25883   Mean   : 3.185   Mean   : 4.242  \n &lt;.$25,000:20135   3rd Qu.: 2.000   3rd Qu.: 3.000  \n &lt;.$20,000:15994   Max.   :30.000   Max.   :30.000  \n (Other)  :21594                                    \n\n\n\n#getting an idea of the count of different levels, this can also provide a general idea of how likely it is for someone to have diabetes under certain conditions; this is further shown through the plots\ndiabetes_data |&gt;\n  group_by(Diabetes_binary, AnyHealthcare, NoDocbcCost, Education, Income, MentHlth, PhysHlth) |&gt;\n  tally()\n\n# A tibble: 19,947 × 8\n# Groups:   Diabetes_binary, AnyHealthcare, NoDocbcCost, Education, Income,\n#   MentHlth [4,311]\n   Diabetes_binary AnyHealthcare NoDocbcCost Education Income  MentHlth PhysHlth\n   &lt;fct&gt;           &lt;fct&gt;         &lt;fct&gt;       &lt;fct&gt;     &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 No              No            No          None      &lt;.$10,…        2       30\n 2 No              No            No          None      &lt;.$15,…        0        0\n 3 No              No            No          None      &lt;.$15,…       30        0\n 4 No              No            No          None      &lt;.$20,…        0        0\n 5 No              No            No          None      &lt;.$20,…        3        7\n 6 No              No            No          None      &lt;.$20,…       30        3\n 7 No              No            No          None      &lt;.$20,…       30       30\n 8 No              No            No          None      &lt;.$25,…        0        5\n 9 No              No            No          None      &lt;.$35,…        0        0\n10 No              No            No          None      &lt;.$35,…        0       30\n# ℹ 19,937 more rows\n# ℹ 1 more variable: n &lt;int&gt;\n\n\n\nWhen looking at the interactions, a vast majority of the population has healthcare, no heavy use of alcohol and costs are not an immediate concrern when going to the doctor. This majority also does not exibit any signs of diabetes. There are logical conclusions that could be drawn from this data alone, such as access to healthcare leading to healthier life styles which lower the chances of diabetes occuring in a population. However, these are just raw numbers. Further analysis would be needed to examine the likelyhood of such an occurance.\n\n\n#taking a simple look at the interaction\nggplot(diabetes_data, aes(x = Diabetes_binary, fill = Income)) +\n    geom_bar(position = \"dodge\") +\n    facet_wrap(~ Education, nrow = 2) \n\n\n\n\n\n\n\n\n\n#taking a closer look at the interaction between education levels and diagnosis\nggplot(diabetes_data, aes(x = Diabetes_binary, fill = Education)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#taking a closer look at the interaction between income and diagnosis\nggplot(diabetes_data, aes(x = Diabetes_binary, fill = Income)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#visualizing population mental health and diabetes diagnisis\nggplot(diabetes_data, aes(x = MentHlth, fill = Diabetes_binary)) + \n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#visualizing population physical health and diabetes diagnisis\nggplot(diabetes_data, aes(x = PhysHlth, fill = Diabetes_binary)) + \n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#adding mental health to other variable visualization\nggplot(diabetes_data, aes(MentHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~ Education, nrow = 2) +\n  theme_bw() +\n  theme(strip.placement = \"outside\",\n        strip.background = element_blank(),\n        panel.border = element_blank(),\n        panel.spacing = unit(0, \"points\"),\n        axis.line = element_line()) + \n  scale_y_continuous(labels = function(y) format(y, scientific = FALSE))\n\n\n\n\n\n\n\n\n\n#repeating with physical health\nggplot(diabetes_data, aes(PhysHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  facet_grid(AnyHealthcare~NoDocbcCost, switch = \"x\", labeller=label_both) +\n  theme_bw() +\n  theme(strip.placement = \"outside\",\n        strip.background = element_blank(),\n        panel.border = element_blank(),\n        panel.spacing = unit(0, \"points\"),\n        axis.line = element_line()) + \n  scale_y_continuous(labels = function(y) format(y, scientific = FALSE))\n\n\n\n\n\n\n\n\n\nWhen looking at the interactions, a vast majority of the population has healthcare, no heavy use of alcohol and costs are not an immediate concrern when going to the doctor. This majority also does not exibit any signs of diabetes. There are logical conclusions that could be drawn from this data alone, such as access to healthcare leading to healthier life styles which lower the chances of diabetes occuring in a population. When taking education and income into account, there is a alight increase in diabetes diagnosis amongst those who only have a high school education or some/technical education. Though a small spike, it is still significant in the larger scheme of things as that could be caused by other socio-economic factors.\n\nModel Fitting"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#initial-library-read-in",
    "href": "Modeling.html#initial-library-read-in",
    "title": "ST558 - Final Project: Modeling",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(Metrics)\nlibrary(ranger)"
  },
  {
    "objectID": "Modeling.html#data",
    "href": "Modeling.html#data",
    "title": "ST558 - Final Project: Modeling",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_model &lt;- read_csv(\"./data/diabetes_data.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): Diabetes_binary, HighBP, HighChol, CholCheck, Smoker, Stroke, Hear...\ndbl  (3): BMI, MentHlth, PhysHlth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndiabetes_model &lt;- diabetes_model |&gt;\n  mutate(across(where(is_character), as_factor))\n\n\n#setting seed for reproducibility\nset.seed(123)\n\n#setting up to split the data into two for later use as training and testing \ntrain &lt;- sample(1:nrow(diabetes_model), size = nrow(diabetes_model)*0.7)\ntest &lt;- setdiff(1:nrow(diabetes_model), train)\n\n#subsetting the data set\ndiabetes_train &lt;- diabetes_model[train, ]\ndiabetes_test &lt;- diabetes_model[test, ]"
  },
  {
    "objectID": "Modeling.html#models",
    "href": "Modeling.html#models",
    "title": "ST558 - Final Project: Modeling",
    "section": "Models:",
    "text": "Models:\n\n#setting up training control for future models\ntrainctrl &lt;- trainControl(method = \"cv\", \n                          number = 5, \n                          summaryFunction = mnLogLoss, \n                          classProbs = TRUE)\n\n\nLogistic Regression:\n\nA type of supervised statistical modeling that predicts the probability of a binary event occuring. Either it happened or it didn’t. Supervised model also means that it cannot generate results. Its application to this set of data relates to both the outcome, a binary response, and the desired parameter of log loss. In logistic regression log likelihood function to determine the beta coefficient which is directly related of log loss.\n\n\nBinomial:\n\n#setting up a seed for reproducing results\nset.seed(124)\n\nbinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth,\n                  data = diabetes_train,\n                  method = \"glm\",\n                  family = binomial,\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainctrl)\n\n#running the model\nbinom\n\nGeneralized Linear Model \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3800039\n\n\n\n\nMultinomial:\n\n#setting up a seed for reproducing results\nset.seed(125)\n\nmultinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth,\n              data = diabetes_train,\n              method = \"multinom\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nmultinom\n\nPenalized Multinomial Regression \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142060, 142061, 142061 \nResampling results across tuning parameters:\n\n  decay  logLoss  \n  0e+00  0.3799973\n  1e-04  0.3799973\n  1e-01  0.3799972\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was decay = 0.1.\n\n\n\n\nBayesian Generalized Linear Model:\n\n#setting up a seed for reproducing results\nset.seed(126)\n\nbayes &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth,\n              data = diabetes_train,\n              method = \"bayesglm\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nbayes\n\nBayesian Generalized Linear Model \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results:\n\n  logLoss  \n  0.3799946\n\n\n\n#visualizing the three models next to each other to find the best one\nrbind(c(\"Binomial\", binom$results[c(\"logLoss\", \"logLossSD\")]), #the best model by a single point \n      c(\"Multinomial\", multinom$results[3, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"Bayesian GLM\", bayes$results[c(\"logLoss\", \"logLossSD\")]) #winning model by 0.000003 of a point\n      )\n\n                    logLoss   logLossSD   \n[1,] \"Binomial\"     0.3800039 0.001242739 \n[2,] \"Multinomial\"  0.3799972 0.001364705 \n[3,] \"Bayesian GLM\" 0.3799946 0.0006920752\n\n\n\n\n\nClassification Tree:\n\nClassification tree are a statistical model used to predict a qualitative response that divide the possible outcomes into distinct regions that do not overlap. For every outcome that falls into a specific region a prediction is made, which is simply the mean of the response variable for the training observations in that region. As binary data is a qualitative data type, this data set is a good match for classification tree model.\n\n\n#setting up a seed for reproducing results\nset.seed(127)\n\n#creating an object for the tuning parameter\ncp &lt;- expand.grid(cp = seq(0, 0.1, 0.01))\n\n#unfortunately, the resulting tree model needed additional variables to get a better fit\nclasstree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree\n\nCART \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3887562\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(128)\n\n#altering the tuning parameter\ncp2 &lt;- expand.grid(cp = seq(0, 0.05, 0.005))\n\nclasstree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp2)\n\n#running the model\nclasstree2\n\nCART \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142060, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3857641\n  0.005  0.4032255\n  0.010  0.4032255\n  0.015  0.4032255\n  0.020  0.4032255\n  0.025  0.4032255\n  0.030  0.4032255\n  0.035  0.4032255\n  0.040  0.4032255\n  0.045  0.4032255\n  0.050  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(129)\n\n#altering the tuning parameter\ncp3 &lt;- expand.grid(cp = seq(0.75, 1, 0.01))\n\nclasstree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree3\n\nCART \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142061, 142060, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3860290\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#visualizing the three models next to each other to find the best level of cp\nrbind(c(\"CP: 0-0.1, 0.01\", classtree$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-0.05, 0.005\", classtree2$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-1, 0.1\", classtree3$results[1, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                         logLoss   logLossSD  \n[1,] \"CP: 0-0.1, 0.01\"   0.3887562 0.00308509 \n[2,] \"CP: 0-0.05, 0.005\" 0.3857641 0.001145128\n[3,] \"CP: 0-1, 0.1\"      0.386029  0.003647874\n\n\n\n\nRandom Forest:\n\nA model where when building decision trees, each time a split is made, the algorithm cannot consider the true number of predictors and instead uses a subsection of the full set. This prevents one strong predictor as being made the main one and gives other predictor variables to play an equal role. As the data in this set has a strong predictor as seen in the EDA, random forset is a good choice of a model.\n\n\n#setting up a seed for reproducing results\nset.seed(130)\n\n#setting up mtry sequence with 11 as the number of predictors\nmtry &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=100,\n                    mtry = seq(1:6))\n\n#building the model with random forest model and training it on the train data set\nrandtree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n                 data = diabetes_train, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry,\n                 ntree = 50,\n                 trControl = trainctrl)\n\n#running the model\nrandtree\n\nRandom Forest \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.3885527\n  2     0.3819272\n  3     0.3799755\n  4     0.3793209\n  5     0.3791845\n  6     0.3792551\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 5, splitrule = extratrees\n and min.node.size = 100.\n\n\n\n#setting up a seed for reproducing results\nset.seed(131)\n\nmtry2 &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=75,\n                    mtry = seq(1:6))\n\n#repeating the model\nrandtree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n                 data = diabetes_train, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry2,\n                 ntree = 50,\n                 trControl = trainctrl)\n\n#running the model\nrandtree2 \n\nRandom Forest \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142061, 142060, 142061 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.3884638\n  2     0.3818999\n  3     0.3800075\n  4     0.3793678\n  5     0.3792473\n  6     0.3793626\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 75\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 5, splitrule = extratrees\n and min.node.size = 75.\n\n\n\n#setting up a seed for reproducing results\nset.seed(132)\n\n#altering mtry\nmtry3 &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=50,\n                    mtry = seq(1:6))\n#repeating the model\nrandtree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + Education + Income + MentHlth + PhysHlth, \n                 data = diabetes_train, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry3,\n                 ntree = 50,\n                 trControl = trainctrl)\n\n#running the model\nrandtree3 \n\nRandom Forest \n\n177576 samples\n     6 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142060, 142061, 142061 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.3885868\n  2     0.3819370\n  3     0.3800090\n  4     0.3793882\n  5     0.3793003\n  6     0.3794912\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 50\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 5, splitrule = extratrees\n and min.node.size = 50.\n\n\n\n#visualizing the three models next to each other to find the best level of mtry and n-trees\nrbind(c(\"mtry: 1-6, nodes: 100\", randtree$results[5, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-6, nodes: 75\", randtree2$results[5, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-6, nodes: 50\", randtree3$results[5, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                             logLoss   logLossSD   \n[1,] \"mtry: 1-6, nodes: 100\" 0.3791845 0.001367659 \n[2,] \"mtry: 1-6, nodes: 75\"  0.3792473 0.0005999624\n[3,] \"mtry: 1-6, nodes: 50\"  0.3793003 0.001136739"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST558 - Final Project: Modeling",
    "section": "Final Model Selection:",
    "text": "Final Model Selection:\n\n#running the model over the test data with probability as the output and building it into a data frame for use in the log loss function\npredicted1 &lt;- data.frame(obs = diabetes_test$Diabetes_binary, #observation values taken from the test data set\n                        pred = predict(bayes, diabetes_test), #the prediction function with raw values \n                        predict(binom, diabetes_test, type = \"prob\")) #probability prediction\n\n#calculating log loss\nglm_test &lt;- mnLogLoss(predicted1, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with classification tree model \npredicted2 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(classtree, diabetes_test),\n                        predict(classtree, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nct_test &lt;- mnLogLoss(predicted2, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with random forest\npredicted3 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(randtree, diabetes_test),\n                        predict(randtree, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nrf_test &lt;- mnLogLoss(predicted3, lev = levels(diabetes_test$Diabetes_binary))"
  },
  {
    "objectID": "Modeling.html#winning-model",
    "href": "Modeling.html#winning-model",
    "title": "ST558 - Final Project: Modeling",
    "section": "Winning Model:",
    "text": "Winning Model:\n\nrbind(c(\"Bayes\", glm_test), #the best model by a single point \n      c(\"Classification Tree\", ct_test),\n      c(\"Random Forest\", rf_test))\n\n                           logLoss            \n[1,] \"Bayes\"               \"0.384544029443957\"\n[2,] \"Classification Tree\" \"0.390821998409196\"\n[3,] \"Random Forest\"       \"0.383616241101218\"\n\n\n\nThe winning model in my opinion is the random forest as it showed the lowest log loss in the training models but also the lowest when using the test set."
  }
]