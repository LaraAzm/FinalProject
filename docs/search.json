[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare and ability/inability to see a physicians due to the cost of the visit. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare and ability/inability to see a physicians due to the cost of the visit. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#initial-library-read-in",
    "href": "EDA.html#initial-library-read-in",
    "title": "ST558 - Final Project: EDA",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(Rmisc)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)"
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "ST558 - Final Project: EDA",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_data &lt;- read_csv(\"./data/diabetes_binary_health_indicators_BRFSS2015.csv\", lazy = FALSE)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n#checking on missing values\nsum(is.na(diabetes_data))\n\n[1] 0\n\n\n\n#saving the edited data set for future use\nwrite_csv(diabetes_data, \"./data/diabetes_data.csv\")\n\n\n#getting an idea of the data set\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              Yes    Yes      Yes          40 Yes    No    \n 2 No              No     No       No           25 Yes    No    \n 3 No              Yes    Yes      Yes          28 No     No    \n 4 No              Yes    No       Yes          27 No     No    \n 5 No              Yes    Yes      Yes          24 No     No    \n 6 No              Yes    Yes      Yes          25 Yes    No    \n 7 No              Yes    No       Yes          30 Yes    No    \n 8 No              Yes    Yes      Yes          25 Yes    No    \n 9 Pre.Yes         Yes    Yes      Yes          30 Yes    No    \n10 No              No     No       Yes          24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;"
  },
  {
    "objectID": "EDA.html#summaries",
    "href": "EDA.html#summaries",
    "title": "ST558 - Final Project: EDA",
    "section": "Summaries:",
    "text": "Summaries:\n\n#getting summary of selected variables, will be useful later on\nselected_cols &lt;- c(\"Diabetes_binary\", \"AnyHealthcare\", \"NoDocbcCost\", \"MentHlth\")\n\nsummary(diabetes_data[selected_cols])\n\n Diabetes_binary  AnyHealthcare NoDocbcCost     MentHlth     \n No     :218334   No : 12417    No :232326   Min.   : 0.000  \n Pre.Yes: 35346   Yes:241263    Yes: 21354   1st Qu.: 0.000  \n                                             Median : 0.000  \n                                             Mean   : 3.185  \n                                             3rd Qu.: 2.000  \n                                             Max.   :30.000  \n\n\n\n#getting an idea of the count of different levels, this can also provide a general idea of how likely it is for someone to have diabetes under certain conditions; this is further shown through the plots\ndiabetes_data |&gt;\n  group_by(Diabetes_binary, MentHlth, AnyHealthcare, NoDocbcCost) |&gt;\n  tally()\n\n# A tibble: 223 × 5\n# Groups:   Diabetes_binary, MentHlth, AnyHealthcare [115]\n   Diabetes_binary MentHlth AnyHealthcare NoDocbcCost      n\n   &lt;fct&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;        &lt;int&gt;\n 1 No                     0 No            No            4935\n 2 No                     0 No            Yes           1834\n 3 No                     0 Yes           No          139351\n 4 No                     0 Yes           Yes           6157\n 5 No                     1 No            No             229\n 6 No                     1 No            Yes            129\n 7 No                     1 Yes           No            6882\n 8 No                     1 Yes           Yes            486\n 9 No                     2 No            No             363\n10 No                     2 No            Yes            192\n# ℹ 213 more rows\n\n\n\n#looking at the interactions between the different categorical variables and the response variable \np1 &lt;- ggplot(diabetes_data, aes(Diabetes_binary, fill = interaction(AnyHealthcare, NoDocbcCost))) + \n      geom_bar(position = \"dodge\")\n\np2 &lt;- ggplot(diabetes_data, aes(Diabetes_binary, fill = interaction(DiffWalk, NoDocbcCost))) + \n      geom_bar(position = \"dodge\")\n\nmultiplot(p1,p2)\n\n\n\n\n\n\n\n\n\nWhen looking at the interactions, a vast majority of the population has healthcare, no difficulty walking and costs are not an immediate concrern when going to the doctor. This majority also does not exibit any signs of diabetes. There are logical conclusions that could be drawn from this data alone, such as access to healthcare leading to healthier life styles which lower the chances of diabetes occuring in a population. However, these are just raw numbers. Further analysis would be needed to examine the likelyhood of such an occurance.\n\n\n#using a different setup to look at the interaction\nggplot(diabetes_data, aes(x = Diabetes_binary, fill = AnyHealthcare)) +\n    geom_bar(position = \"dodge\") +\n    facet_grid(.~NoDocbcCost, labeller=label_both) \n\n\n\n\n\n\n\n\n\n#visualizing population mental health and diabetes diagnisis\nggplot(diabetes_data, aes(x = MentHlth, fill = Diabetes_binary)) + \n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#adding mental health to other variable visualization\nggplot(diabetes_data, aes(MentHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  facet_grid(AnyHealthcare~NoDocbcCost, switch = \"x\", labeller=label_both) +\n  theme_bw() +\n  theme(strip.placement = \"outside\",\n        strip.background = element_blank(),\n        panel.border = element_blank(),\n        panel.spacing = unit(0, \"points\"),\n        axis.line = element_line()) + \n  scale_y_continuous(labels = function(y) format(y, scientific = FALSE))\n\n\n\n\n\n\n\n\nModel Fitting"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#initial-library-read-in",
    "href": "Modeling.html#initial-library-read-in",
    "title": "ST558 - Final Project: Modeling",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(Metrics)\nlibrary(ranger)"
  },
  {
    "objectID": "Modeling.html#data",
    "href": "Modeling.html#data",
    "title": "ST558 - Final Project: Modeling",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_model &lt;- read_csv(\"./data/diabetes_data.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): Diabetes_binary, HighBP, HighChol, CholCheck, Smoker, Stroke, Hear...\ndbl  (3): BMI, MentHlth, PhysHlth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndiabetes_model &lt;- diabetes_model |&gt;\n  mutate(across(where(is_character), as_factor))\n\n\n#setting seed for reproducibility\nset.seed(123)\n\n#setting up to split the data into two for later use as training and testing \ntrain &lt;- sample(1:nrow(diabetes_model), size = nrow(diabetes_model)*0.7)\ntest &lt;- setdiff(1:nrow(diabetes_model), train)\n\n#subsetting the data set\ndiabetes_train &lt;- diabetes_model[train, ]\ndiabetes_test &lt;- diabetes_model[test, ]"
  },
  {
    "objectID": "Modeling.html#models",
    "href": "Modeling.html#models",
    "title": "ST558 - Final Project: Modeling",
    "section": "Models:",
    "text": "Models:\n\n#setting up training control for future models\ntrainctrl &lt;- trainControl(method = \"cv\", \n                          number = 5, \n                          summaryFunction = mnLogLoss, \n                          classProbs = TRUE)\n\n\nLogistic Regression:\n\nA type of supervised statistical modeling that predicts the probability of a binary event occuring. Either it happened or it didn’t. Supervised model also means that it cannot generate results. Its application to this set of data relates to both the outcome, a binary response, and the desired parameter of log loss. In logistic regression log likelihood function to determine the beta coefficient which is directly related of log loss.\n\n\nBinomial:\n\n#setting up a seed for reproducing results\nset.seed(124)\n\nbinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n                  data = diabetes_model,\n                  method = \"glm\",\n                  family = binomial,\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainctrl)\n\n#running the model\nbinom\n\nGeneralized Linear Model \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202945, 202944, 202944, 202944, 202943 \nResampling results:\n\n  logLoss  \n  0.4011343\n\n\n\n\nMultinomial:\n\n#setting up a seed for reproducing results\nset.seed(125)\n\nmultinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n              data = diabetes_model,\n              method = \"multinom\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nmultinom\n\nPenalized Multinomial Regression \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202945, 202944, 202944, 202943 \nResampling results across tuning parameters:\n\n  decay  logLoss  \n  0e+00  0.4011445\n  1e-04  0.4011445\n  1e-01  0.4011446\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was decay = 0.\n\n\n\n\nBayesian Generalized Linear Model:\n\n#setting up a seed for reproducing results\nset.seed(126)\n\nbayes &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n              data = diabetes_model,\n              method = \"bayesglm\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nbayes\n\nBayesian Generalized Linear Model \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202943, 202944, 202944, 202945, 202944 \nResampling results:\n\n  logLoss  \n  0.4011344\n\n\n\n#visualizing the three models next to each other to find the best one\nrbind(c(\"Binomial\", binom$results[c(\"logLoss\", \"logLossSD\")]), #the best model by a single point \n      c(\"Multinomial\", multinom$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"Bayesian GLM\", bayes$results[c(\"logLoss\", \"logLossSD\")])\n      )\n\n                    logLoss   logLossSD   \n[1,] \"Binomial\"     0.4011343 0.0001439749\n[2,] \"Multinomial\"  0.4011445 0.0004610336\n[3,] \"Bayesian GLM\" 0.4011344 0.0003495619\n\n\n\n\n\nClassification Tree:\n\nClassification tree are a statistical model used to predict a qualitative response that divide the possible outcomes into distinct regions that do not overlap. For every outcome that falls into a specific region a prediction is made, which is simply the mean of the response variable for the training observations in that region. As binary data is a qualitative data type, this data set is a good match for classification tree model.\n\n\n#setting up a seed for reproducing results\nset.seed(127)\n\n#creating an object for the tuning parameter\ncp &lt;- expand.grid(cp = seq(0, 0.1, 0.01))\n\n#unfortunately, the resulting tree model needed additional variables to get a better fit\nclasstree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3745657\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(128)\n\n#altering the tuning parameter\ncp2 &lt;- expand.grid(cp = seq(0, 0.5, 0.05))\n\nclasstree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp2)\n\n#running the model\nclasstree2\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142060, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3805502\n  0.05  0.4032255\n  0.10  0.4032255\n  0.15  0.4032255\n  0.20  0.4032255\n  0.25  0.4032255\n  0.30  0.4032255\n  0.35  0.4032255\n  0.40  0.4032255\n  0.45  0.4032255\n  0.50  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(129)\n\n#altering the tuning parameter\ncp3 &lt;- expand.grid(cp = seq(0, 1, 0.1))\n\nclasstree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree3\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142061, 142060, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3752732\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#visualizing the three models next to each other to find the best level of cp\nrbind(c(\"CP: 0-0.1, 0.01, \", classtree$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-0.5, 0.05\", classtree2$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-1, 0.1\", classtree3$results[1, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                         logLoss   logLossSD  \n[1,] \"CP: 0-0.1, 0.01, \" 0.3745657 0.003493501\n[2,] \"CP: 0-0.5, 0.05\"   0.3805502 0.003567191\n[3,] \"CP: 0-1, 0.1\"      0.3752732 0.005795191\n\n\n\n\nRandom Forest:\n\nA model where when building decision trees, each time a split is made, the algorithm cannot consider the true number of predictors and instead uses a subsection of the full set. This prevents one strong predictor as being made the main one and gives other predictor variables to play an equal role. As the data in this set has a strong predictor as seen in the EDA, random forset is a good choice of a model.\n\n\n#setting up a seed for reproducing results\nset.seed(130)\n\n#setting up mtry sequence with 11 as the number of predictors\nmtry &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=100,\n                    mtry = seq(1:3))\n\n#building the model with random forest model and training it on the train data set\nrandtree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry,\n                 ntree = 50,\n                 trControl = trainctrl)\n\n#running the model\nrandtree\n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202945, 202944, 202943, 202944, 202944 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.4013815\n  2     0.4008127\n  3     0.4008310\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 100.\n\n\n\n#setting up a seed for reproducing results\nset.seed(131)\n\nmtry2 &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=100,\n                    mtry = seq(1:3))\n\n#repeating the model\nrandtree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry2,\n                 ntree = 75,\n                 trControl = trainctrl)\n\n#running the model\nrandtree2 \n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202943, 202944, 202944, 202945 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.4014021\n  2     0.4008260\n  3     0.4008941\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 100.\n\n\n\n#setting up a seed for reproducing results\nset.seed(132)\n\n#altering mtry\nmtry3 &lt;- expand.grid(splitrule=\"extratrees\",\n                    min.node.size=100,\n                    mtry = seq(1:3))\n#repeating the model\nrandtree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"ranger\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry3,\n                 ntree = 100,\n                 trControl = trainctrl)\n\n#running the model\nrandtree3 \n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202944, 202944, 202944, 202944 \nResampling results across tuning parameters:\n\n  mtry  logLoss  \n  1     0.4013931\n  2     0.4008142\n  3     0.4008548\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\n\nTuning parameter 'min.node.size' was held constant at a value of 100\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 100.\n\n\n\n#visualizing the three models next to each other to find the best level of mtry and n-trees\nrbind(c(\"mtry: 1-5, n-tree: 50\", randtree$results[2, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-7, n-tree: 75\", randtree2$results[2, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-10, n-tree: 100\", randtree3$results[2, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                               logLoss   logLossSD   \n[1,] \"mtry: 1-5, n-tree: 50\"   0.4008127 0.0002007077\n[2,] \"mtry: 1-7, n-tree: 75\"   0.400826  0.0002944988\n[3,] \"mtry: 1-10, n-tree: 100\" 0.4008142 0.000377963"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST558 - Final Project: Modeling",
    "section": "Final Model Selection:",
    "text": "Final Model Selection:\n\n#running the model over the test data with probability as the output and building it into a data frame for use in the log loss function\npredicted1 &lt;- data.frame(obs = diabetes_test$Diabetes_binary, #observation values taken from the test data set\n                        pred = predict(binom, diabetes_test), #the prediction function with raw values \n                        predict(binom, diabetes_test, type = \"prob\")) #probability prediction\n\n#calculating log loss\nglm_test &lt;- mnLogLoss(predicted1, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with classification tree model \npredicted2 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(classtree, diabetes_test),\n                        predict(classtree, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nct_test &lt;- mnLogLoss(predicted2, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with random forest\npredicted3 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(randtree, diabetes_test),\n                        predict(randtree, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nrf_test &lt;- mnLogLoss(predicted3, lev = levels(diabetes_test$Diabetes_binary))"
  },
  {
    "objectID": "Modeling.html#winning-model",
    "href": "Modeling.html#winning-model",
    "title": "ST558 - Final Project: Modeling",
    "section": "Winning Model:",
    "text": "Winning Model:\n\nrbind(c(\"Binomial\", glm_test), #the best model by a single point \n      c(\"Classification Tree\", ct_test),\n      c(\"Random Forest\", rf_test))\n\n                           logLoss            \n[1,] \"Binomial\"            \"0.403094104655764\"\n[2,] \"Classification Tree\" \"0.378453507051412\"\n[3,] \"Random Forest\"       \"0.402505402837576\"\n\n\n\nThough the lowest log loss was shown by the classification tree model, that was achieved by adding more than the chosen response variables to coax the model to give different values. The winning model in my opinion is the random forest as it showed the lowest log loss in the training models but also the lowest when using the test set."
  }
]