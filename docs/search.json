[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare and ability/inability to see a physicians due to the cost of the visit. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST558 - Final Project: EDA",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare and ability/inability to see a physicians due to the cost of the visit. The response variable is the presence of prediabetes or diabetes. The purpose of this EDA is to get a general idea of how the chosen variables effect levels of diabetes. Ultimately, I will be using these variables to model the likelyhood of diabetes occuring at different levels of variables."
  },
  {
    "objectID": "EDA.html#initial-library-read-in",
    "href": "EDA.html#initial-library-read-in",
    "title": "ST558 - Final Project: EDA",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(Rmisc)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(scales)"
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "ST558 - Final Project: EDA",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_data &lt;- read_csv(\"./data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n#checking on missing values\nsum(is.na(diabetes_data))\n\n[1] 0\n\n\n\n#getting an idea of the data set\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP  HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;   &lt;fct&gt;    &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              Yes HBP Yes HiC  Yes CC       40 Yes Sm No St \n 2 No              No HPB  No HiC   No CC        25 Yes Sm No St \n 3 No              Yes HBP Yes HiC  Yes CC       28 No Sm  No St \n 4 No              Yes HBP No HiC   Yes CC       27 No Sm  No St \n 5 No              Yes HBP Yes HiC  Yes CC       24 No Sm  No St \n 6 No              Yes HBP Yes HiC  Yes CC       25 Yes Sm No St \n 7 No              Yes HBP No HiC   Yes CC       30 Yes Sm No St \n 8 No              Yes HBP Yes HiC  Yes CC       25 Yes Sm No St \n 9 Pre/Yes         Yes HBP Yes HiC  Yes CC       30 Yes Sm No St \n10 No              No HPB  No HiC   Yes CC       24 No Sm  No St \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;"
  },
  {
    "objectID": "EDA.html#summaries",
    "href": "EDA.html#summaries",
    "title": "ST558 - Final Project: EDA",
    "section": "Summaries:",
    "text": "Summaries:\n\n#getting an idea of the count of different levels, this can also provide a general idea of how likely it is for someone to have diabetes under certain conditions; this is further shown through the plots\ndiabetes_data |&gt;\n  group_by(Diabetes_binary, MentHlth, AnyHealthcare, NoDocbcCost) |&gt;\n  tally()\n\n# A tibble: 223 × 5\n# Groups:   Diabetes_binary, MentHlth, AnyHealthcare [115]\n   Diabetes_binary MentHlth AnyHealthcare NoDocbcCost      n\n   &lt;fct&gt;              &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;        &lt;int&gt;\n 1 No                     0 No HC         No Doc        4935\n 2 No                     0 No HC         Yes Doc       1834\n 3 No                     0 Yes HC        No Doc      139351\n 4 No                     0 Yes HC        Yes Doc       6157\n 5 No                     1 No HC         No Doc         229\n 6 No                     1 No HC         Yes Doc        129\n 7 No                     1 Yes HC        No Doc        6882\n 8 No                     1 Yes HC        Yes Doc        486\n 9 No                     2 No HC         No Doc         363\n10 No                     2 No HC         Yes Doc        192\n# ℹ 213 more rows\n\n\n\n#looking at the interactions between the different categorical variables and the response variable \np1 &lt;- ggplot(diabetes_data, aes(Diabetes_binary, fill = interaction(AnyHealthcare, NoDocbcCost))) + \n      geom_bar(position = \"dodge\")\n\np2 &lt;- ggplot(diabetes_data, aes(Diabetes_binary, fill = interaction(DiffWalk, NoDocbcCost))) + \n      geom_bar(position = \"dodge\")\n\nmultiplot(p1,p2)\n\n\n\n\n\n\n\n\n\nWhen looking at the interactions, a vast majority of the population has healthcare, no difficulty walking and costs are not an immediate concrern when going to the doctor. This majority also does not exibit any signs of diabetes. There are logical conclusions that could be drawn from this data alone, such as access to healthcare leading to healthier life styles which lower the chances of diabetes occuring in a population. However, these are just raw numbers. Further analysis would be needed to examine the likelyhood of such an occurance.\n\n\n#using a different setup to look at the interaction\nggplot(diabetes_data, aes(x = Diabetes_binary, fill = AnyHealthcare)) +\n    geom_bar(position = \"dodge\") +\n    facet_grid(.~NoDocbcCost, labeller=label_both) \n\n\n\n\n\n\n\n\n\n#visualizing population mental health and diabetes diagnisis\nggplot(diabetes_data, aes(x = MentHlth, fill = Diabetes_binary)) + \n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n#adding mental health to other variable visualization\nggplot(diabetes_data, aes(MentHlth, fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  facet_grid(AnyHealthcare~NoDocbcCost, switch = \"x\", labeller=label_both) +\n  theme_bw() +\n  theme(strip.placement = \"outside\",\n        strip.background = element_blank(),\n        panel.border = element_blank(),\n        panel.spacing = unit(0, \"points\"),\n        axis.line = element_line()) + \n  scale_y_continuous(labels = function(y) format(y, scientific = FALSE))\n\n\n\n\n\n\n\n\nModel Fitting"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST558 - Final Project: Modeling",
    "section": "",
    "text": "The base data set for this workthrough is a binary data set of survey responses to a CDC behavioral and chronic disease surveillance study done for diabetes. I have chosen mental health, healthcare, ability/inability to see a physicians due to the cost of the visit and difficulty walking as my variables. The response variable is the presence of prediabetes or diabetes. Here, I will be modeling using log loss. Log loss is the negative of log of likelihood, a function denoting the likelihood a model believes the real observed values could occur. Accuracy can only be applied to classification tasks and while easier to interpret, higher number means higher accuracy, its limitations mean that it cannot be applied to regression tasks. Log loss is best used for binary classifications because of the connection between the likelihood function and if the response variable is encoded as 0 or 1."
  },
  {
    "objectID": "Modeling.html#initial-library-read-in",
    "href": "Modeling.html#initial-library-read-in",
    "title": "ST558 - Final Project: Modeling",
    "section": "Initial Library Read-in:",
    "text": "Initial Library Read-in:\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(Metrics)"
  },
  {
    "objectID": "Modeling.html#data",
    "href": "Modeling.html#data",
    "title": "ST558 - Final Project: Modeling",
    "section": "Data:",
    "text": "Data:\n\n#reading in the csv \ndiabetes_model &lt;- read_csv(\"./data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n#setting seed for reproducability\nset.seed(123)\n\n#setting up to split the data into two for later use as training and testing \ntrain &lt;- sample(1:nrow(diabetes_model), size = nrow(diabetes_model)*0.7)\ntest &lt;- setdiff(1:nrow(diabetes_model), train)\n\n#subsetting the data set\ndiabetes_train &lt;- diabetes_model[train, ]\ndiabetes_test &lt;- diabetes_model[test, ]"
  },
  {
    "objectID": "Modeling.html#models",
    "href": "Modeling.html#models",
    "title": "ST558 - Final Project: Modeling",
    "section": "Models:",
    "text": "Models:\n\n#setting up training control for future models\ntrainctrl &lt;- trainControl(method = \"cv\", \n                          number = 5, \n                          summaryFunction = mnLogLoss, \n                          classProbs = TRUE)\n\n\nLogistic Regression:\n\nA type of supervised statistical modeling that predicts the probability of a binary event occuring. Either it happened or it didn’t. Supervised model also means that it cannot generate results. Its application to this set of data relates to both the outcome, a binary response, and the desired parameter of log loss. In logistic regression log likelihood function to determine the beta coefficient which is directly related of log loss.\n\n\nBinomial:\n\n#setting up a seed for reproducing results\nset.seed(124)\n\nbinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n                  data = diabetes_model,\n                  method = \"glm\",\n                  family = binomial,\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainctrl)\n\n#running the model\nbinom\n\nGeneralized Linear Model \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202945, 202944, 202944, 202944, 202943 \nResampling results:\n\n  logLoss  \n  0.4011343\n\n\n\n\nMultinomial:\n\n#setting up a seed for reproducing results\nset.seed(125)\n\nmultinom &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n              data = diabetes_model,\n              method = \"multinom\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nmultinom\n\nPenalized Multinomial Regression \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202945, 202944, 202944, 202943 \nResampling results across tuning parameters:\n\n  decay  logLoss  \n  0e+00  0.4011445\n  1e-04  0.4011445\n  1e-01  0.4011446\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was decay = 0.\n\n\n\n\nBayesian Generalized Linear Model:\n\n#setting up a seed for reproducing results\nset.seed(126)\n\nbayes &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth,\n              data = diabetes_model,\n              method = \"bayesglm\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              trace = FALSE)\n\n#running the model\nbayes\n\nBayesian Generalized Linear Model \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202943, 202944, 202944, 202945, 202944 \nResampling results:\n\n  logLoss  \n  0.4011344\n\n\n\n#visualizing the three models next to each other to find the best one\nrbind(c(\"Binomial\", binom$results[c(\"logLoss\", \"logLossSD\")]), #the best model by a single point \n      c(\"Multinomial\", multinom$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"Bayesian GLM\", bayes$results[c(\"logLoss\", \"logLossSD\")])\n      )\n\n                    logLoss   logLossSD   \n[1,] \"Binomial\"     0.4011343 0.0001439749\n[2,] \"Multinomial\"  0.4011445 0.0004610336\n[3,] \"Bayesian GLM\" 0.4011344 0.0003495619\n\n\n\n\n\nClassification Tree:\n\nClassification tree are a statistical model used to predict a qualitative response that divide the possible outcomes into distinct regions that do not overlap. For every outcome that falls into a specific region a prediction is made, which is simply the mean of the response variable for the training observations in that region. As binary data is a qualitative data type, this data set is a good match for classification tree model.\n\n\n#setting up a seed for reproducing results\nset.seed(127)\n\n#creating an object for the tuning parameter\ncp &lt;- expand.grid(cp = seq(0, 0.1, 0.01))\n\n#unfortunately, the resulting tree model needed additional variables to get a better fit\nclasstree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142060, 142061, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3795635\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(128)\n\n#altering the tuning parameter\ncp2 &lt;- expand.grid(cp = seq(0, 0.5, 0.05))\n\nclasstree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp2)\n\n#running the model\nclasstree2\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142060, 142061, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3835350\n  0.05  0.4032255\n  0.10  0.4032255\n  0.15  0.4032255\n  0.20  0.4032255\n  0.25  0.4032255\n  0.30  0.4032255\n  0.35  0.4032255\n  0.40  0.4032255\n  0.45  0.4032255\n  0.50  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#setting up a seed for reproducing results\nset.seed(129)\n\n#altering the tuning parameter\ncp3 &lt;- expand.grid(cp = seq(0, 1, 0.1))\n\nclasstree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth + Age + BMI + PhysHlth + Education, \n              data = diabetes_train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              trControl = trainctrl,\n              preProcess=c(\"center\",\"scale\"),\n              tuneGrid = cp)\n\n#running the model\nclasstree3\n\nCART \n\n177576 samples\n     7 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (22), scaled (22) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142061, 142060, 142061 \nResampling results across tuning parameters:\n\n  cp    logLoss  \n  0.00  0.3805030\n  0.01  0.4032255\n  0.02  0.4032255\n  0.03  0.4032255\n  0.04  0.4032255\n  0.05  0.4032255\n  0.06  0.4032255\n  0.07  0.4032255\n  0.08  0.4032255\n  0.09  0.4032255\n  0.10  0.4032255\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\n\n#visualizing the three models next to each other to find the best level of cp\nrbind(c(\"CP: 0-0.1, 0.01, \", classtree$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-0.5, 0.05\", classtree2$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"CP: 0-1, 0.1\", classtree3$results[1, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                         logLoss   logLossSD  \n[1,] \"CP: 0-0.1, 0.01, \" 0.3795635 0.006394978\n[2,] \"CP: 0-0.5, 0.05\"   0.383535  0.004933967\n[3,] \"CP: 0-1, 0.1\"      0.380503  0.008508897\n\n\n\n\nRandom Forest:\n\nA model where when building decision trees, each time a split is made, the algorithm cannot consider the true number of predictors and instead uses a subsection of the full set. This prevents one strong predictor as being made the main one and gives other predictor variables to play an equal role. As the data in this set has a strong predictor as seen in the EDA, random forset is a good choice of a model.\n\n\n#setting up a seed for reproducing results\nset.seed(130)\n\n#setting up mtry sequence with 11 as the number of predictors\nmtry &lt;- expand.grid(mtry = seq(1:5))\n\n#building the model with random forest model and training it on the train data set\nrandtree &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"rf\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry,\n                 ntree = 50,\n                 trControl = trainctrl)\n\n#running the model\nrandtree \n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202945, 202944, 202943, 202944, 202944 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.811912\n  2     4.808108\n  3     4.808099\n  4     4.807734\n  5     4.806371\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5.\n\n#the model is completely unhelpful\n\n\n#setting up a seed for reproducing results\nset.seed(131)\n\n#altering mtry\nmtry2 &lt;- expand.grid(mtry = seq(1:7))\n\n#repeating the model\nrandtree2 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"rf\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry2,\n                 ntree = 75,\n                 trControl = trainctrl)\n\n#running the model\nrandtree2 \n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202943, 202944, 202944, 202945 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.811914\n  2     4.806610\n  3     4.805013\n  4     4.804940\n  5     4.805007\n  6     4.804786\n  7     4.805645\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 6.\n\n\n\n#setting up a seed for reproducing results\nset.seed(132)\n\n#altering mtry\nmtry3 &lt;- expand.grid(mtry = seq(1:10))\n\n#repeating the model\nrandtree3 &lt;- train(Diabetes_binary ~ AnyHealthcare + NoDocbcCost + MentHlth, \n                 data = diabetes_model, \n                 method = \"rf\",\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = mtry3,\n                 ntree = 100,\n                 trControl = trainctrl)\n\n#running the model\nrandtree3 \n\nRandom Forest \n\n253680 samples\n     3 predictor\n     2 classes: 'No', 'Pre.Yes' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 202944, 202944, 202944, 202944, 202944 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n   1    4.811212\n   2    4.808026\n   3    4.805863\n   4    4.805138\n   5    4.804889\n   6    4.805136\n   7    4.805727\n   8    4.805573\n   9    4.806345\n  10    4.806105\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5.\n\n\n\n#visualizing the three models next to each other to find the best level of mtry and n-trees\nrbind(c(\"mtry: 1-5, n-tree: 50, \", randtree$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-7, n-tree: 75\", randtree2$results[1, ][c(\"logLoss\", \"logLossSD\")]),\n      c(\"mtry: 1-10, n-tree: 100\", randtree3$results[1, ][c(\"logLoss\", \"logLossSD\")])\n      )\n\n                               logLoss  logLossSD   \n[1,] \"mtry: 1-5, n-tree: 50, \" 4.811912 0.0005274459\n[2,] \"mtry: 1-7, n-tree: 75\"   4.811914 0.0004973519\n[3,] \"mtry: 1-10, n-tree: 100\" 4.811212 0.001375305"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST558 - Final Project: Modeling",
    "section": "Final Model Selection:",
    "text": "Final Model Selection:\n\n#running the model over the test data with probability as the output and building it into a data frame for use in the log loss function\npredicted1 &lt;- data.frame(obs = diabetes_test$Diabetes_binary, #observation values taken from the test data set\n                        pred = predict(binom, diabetes_test), #the prediction function with raw values \n                        predict(binom, diabetes_test, type = \"prob\")) #probability prediction\n\n#calculating log loss\nglm_test &lt;- mnLogLoss(predicted1, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with classification tree model \npredicted2 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(classtree, diabetes_test),\n                        predict(classtree, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nct_test &lt;- mnLogLoss(predicted2, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#repeating with random forest\npredicted3 &lt;- data.frame(obs = diabetes_test$Diabetes_binary,\n                        pred = predict(randtree, diabetes_test),\n                        predict(randtree3, diabetes_test, type = \"prob\"))\n\n#calculating log loss\nrf_test &lt;- mnLogLoss(predicted3, lev = levels(diabetes_test$Diabetes_binary))\n\n\n#viewing the summary to see how truly unbalanced the results from the random forest were and nearly all the values were predicted as `No`\nsummary(predict(randtree3, diabetes_test, type = \"prob\"))\n\n       No            Pre.Yes        \n Min.   :0.4600   Min.   :0.00e+00  \n 1st Qu.:1.0000   1st Qu.:0.00e+00  \n Median :1.0000   Median :0.00e+00  \n Mean   :0.9999   Mean   :7.79e-05  \n 3rd Qu.:1.0000   3rd Qu.:0.00e+00  \n Max.   :1.0000   Max.   :5.40e-01"
  },
  {
    "objectID": "Modeling.html#winning-model",
    "href": "Modeling.html#winning-model",
    "title": "ST558 - Final Project: Modeling",
    "section": "Winning Model:",
    "text": "Winning Model:\n\nrbind(c(\"Binomial\", glm_test), #the best model by a single point \n      c(\"Classification Tree\", ct_test),\n      c(\"Random Forest\", rf_test))\n\n                           logLoss            \n[1,] \"Binomial\"            \"0.403094104655764\"\n[2,] \"Classification Tree\" \"0.378920628279181\"\n[3,] \"Random Forest\"       \"4.82538329812585\" \n\n\n\nThrough the lowest log loss was shown by the classification tree model, that was achieved by adding more than the chosen response variables to coax the model to give different values. The winning model in my opinion is the basic binomial generalized linear model as it showed the lowest log loss in the training models but also the lowest when using the test set."
  }
]